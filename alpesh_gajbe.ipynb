{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e65923-1e19-4449-b43e-376e3a4ccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random as rn\n",
    "import datetime\n",
    "\n",
    "# If using imageio or skimage, install or import accordingly:\n",
    "from imageio import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9baf4e-df60-49ef-9bee-4669cd03cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 13:43:18.667026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735632798.679732   53324 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735632798.683383   53324 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 13:43:18.698121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility across modules\n",
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)  # For older TF versions; in TF 2.x, use tf.random.set_seed(30)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # For memory growth\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99dddb1-6cfd-4795-adcf-d6224a5ec9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to training and validation CSV files\n",
    "train_csv = 'Project_data/train.csv'\n",
    "val_csv   = 'Project_data/val.csv'\n",
    "\n",
    "# Paths to folders containing subfolders of frames\n",
    "train_path = 'Project_data/train'\n",
    "val_path   = 'Project_data/val'\n",
    "\n",
    "# Read CSV lines, then shuffle\n",
    "train_doc = np.random.permutation(open(train_csv).readlines())\n",
    "val_doc   = np.random.permutation(open(val_csv).readlines())\n",
    "\n",
    "# Number of classes (gestures): 5 (Thumbs up/down, Left/Right swipe, Stop)\n",
    "num_classes = 5\n",
    "\n",
    "# Adjust as needed\n",
    "batch_size  = 8 \n",
    "num_epochs  = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75dc065f-7f97-4528-85e5-c6c2d320b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    \"\"\"\n",
    "    source_path: path to train/val folder\n",
    "    folder_list: list of lines from train.csv or val.csv\n",
    "    batch_size : how many videos per batch\n",
    "    \"\"\"\n",
    "    print('Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    # Example: pick frames [0, 2, 4, ... 28] => 15 frames out of 30\n",
    "    # If you want all 30 frames, just do: img_idx = list(range(30))\n",
    "    #img_idx = [x for x in range(0, 30, 2)]\n",
    "    img_idx = list(range(30))\n",
    "    \n",
    "    # Decide final image size (after cropping & resizing)\n",
    "    final_height = 64\n",
    "    final_width  = 64\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle the folder list (videos) every epoch\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list) // batch_size  # number of full batches\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            # Prepare batch arrays\n",
    "            batch_data = np.zeros((batch_size, \n",
    "                                   len(img_idx), \n",
    "                                   final_height, \n",
    "                                   final_width, \n",
    "                                   3))  # shape: (B, T, H, W, C)\n",
    "            batch_labels = np.zeros((batch_size, num_classes))  # one-hot (5 classes)\n",
    "            \n",
    "            for folder in range(batch_size):\n",
    "                # Parse CSV line\n",
    "                folder_line = t[batch * batch_size + folder].strip().split(';')\n",
    "                folder_name = folder_line[0]\n",
    "                gesture_label = int(folder_line[2])  # label 0..4\n",
    "                \n",
    "                # Path to subfolder (which contains frames)\n",
    "                folder_path = os.path.join(source_path, folder_name)\n",
    "                # Ensure frames are sorted if needed\n",
    "                imgs = sorted(os.listdir(folder_path))\n",
    "                \n",
    "                for idx, frame_num in enumerate(img_idx):\n",
    "                    image_path = os.path.join(folder_path, imgs[frame_num])\n",
    "                    image = imread(image_path).astype(np.float32)\n",
    "                    \n",
    "                    # Resize or crop as needed\n",
    "                    image_resized = resize(image, (final_height, final_width))\n",
    "                    # Normalize: scale to [0,1]\n",
    "                    image_resized /= 255.0\n",
    "                    \n",
    "                    # Assign to batch_data\n",
    "                    batch_data[folder, idx, :, :, :] = image_resized\n",
    "                \n",
    "                # One-hot label\n",
    "                batch_labels[folder, gesture_label] = 1\n",
    "            \n",
    "            yield batch_data, batch_labels\n",
    "        \n",
    "        # Handle leftover samples if any\n",
    "        leftover = len(folder_list) % batch_size\n",
    "        if leftover != 0:\n",
    "            batch_data = np.zeros((leftover, \n",
    "                                   len(img_idx), \n",
    "                                   final_height, \n",
    "                                   final_width, \n",
    "                                   3))\n",
    "            batch_labels = np.zeros((leftover, num_classes))\n",
    "            \n",
    "            start_idx = num_batches * batch_size\n",
    "            for folder in range(leftover):\n",
    "                folder_line = t[start_idx + folder].strip().split(';')\n",
    "                folder_name = folder_line[0]\n",
    "                gesture_label = int(folder_line[2])\n",
    "                \n",
    "                folder_path = os.path.join(source_path, folder_name)\n",
    "                imgs = sorted(os.listdir(folder_path))\n",
    "                \n",
    "                for idx, frame_num in enumerate(img_idx):\n",
    "                    image_path = os.path.join(folder_path, imgs[frame_num])\n",
    "                    image = imread(image_path).astype(np.float32)\n",
    "                    \n",
    "                    image_resized = resize(image, (final_height, final_width))\n",
    "                    image_resized /= 255.0\n",
    "                    \n",
    "                    batch_data[folder, idx, :, :, :] = image_resized\n",
    "                \n",
    "                batch_labels[folder, gesture_label] = 1\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fa5782-ce0f-4204-a529-ad28676cf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpesh/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1735632800.593771   53324 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2273 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,696</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m) │           \u001b[38;5;34m448\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m) │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m8192\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m8192\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │       \u001b[38;5;34m789,696\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">797,221</span> (3.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m797,221\u001b[0m (3.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">797,221</span> (3.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m797,221\u001b[0m (3.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense, GRU, TimeDistributed,\n",
    "    Conv2D, MaxPooling2D, Flatten,\n",
    "    Dropout\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "# Example: Conv2D + GRU architecture using TimeDistributed\n",
    "# Input shape = (None, T, H, W, C) => T=number of frames\n",
    "input_frames = 15   # we used 15 frames if img_idx = [0,2,4,...,28]\n",
    "height = 64\n",
    "width  = 64\n",
    "channels = 3\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(16, (3,3), padding='same', activation='relu'),\n",
    "    input_shape=(input_frames, height, width, channels))\n",
    ")\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu')\n",
    "))\n",
    "model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "# Flatten + Dropout to regularize\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# GRU with dropout\n",
    "model.add(GRU(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Dense + optional dropout\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Change 'lr' to 'learning_rate'\n",
    "optimiser = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimiser, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f47bde0-02f2-47de-b246-f497d881a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "num_val_sequences   = len(val_doc)\n",
    "\n",
    "# Steps per epoch\n",
    "if (num_train_sequences % batch_size) == 0:\n",
    "    steps_per_epoch = num_train_sequences // batch_size\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences // batch_size) + 1\n",
    "\n",
    "if (num_val_sequences % batch_size) == 0:\n",
    "    validation_steps = num_val_sequences // batch_size\n",
    "else:\n",
    "    validation_steps = (num_val_sequences // batch_size) + 1\n",
    "\n",
    "# Instantiate our generators\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator   = generator(val_path, val_doc, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3f877a-fce8-4846-9131-b7e305a20524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "# In newer TF/Keras, use .keras extension. If you want .h5, you can rename it,\n",
    "# but you'd have to adjust your Keras version or pass additional arguments.\n",
    "filepath = (\n",
    "    model_name\n",
    "    + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-'\n",
    "    + '{val_loss:.5f}-{val_categorical_accuracy:.5f}.keras'\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "LR = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting if val_loss doesn't improve\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, LR, earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe0e3cb-9f9a-4b0d-b183-22f160a5e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53324/434498319.py:45: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(image_path).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735632807.657745   53538 cuda_dnn.cc:529] Loaded cuDNN version 90600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 780ms/step - categorical_accuracy: 0.2235 - loss: 5.8455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53324/434498319.py:81: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imread(image_path).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778ms/step - categorical_accuracy: 0.2236 - loss: 5.8126Source path =  Project_data/val ; batch size = 8\n",
      "\n",
      "Epoch 1: saving model to model_init_2024-12-3113_43_21.249773/model-00001-3.11572-0.22926-1.63882-0.18000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 932ms/step - categorical_accuracy: 0.2237 - loss: 5.7805 - val_categorical_accuracy: 0.1800 - val_loss: 1.6388 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757ms/step - categorical_accuracy: 0.2284 - loss: 1.5809\n",
      "Epoch 2: saving model to model_init_2024-12-3113_43_21.249773/model-00002-1.57177-0.24887-1.60706-0.18000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 876ms/step - categorical_accuracy: 0.2287 - loss: 1.5808 - val_categorical_accuracy: 0.1800 - val_loss: 1.6071 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720ms/step - categorical_accuracy: 0.3198 - loss: 1.4750\n",
      "Epoch 3: saving model to model_init_2024-12-3113_43_21.249773/model-00003-1.43658-0.36199-1.52759-0.16000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 841ms/step - categorical_accuracy: 0.3203 - loss: 1.4745 - val_categorical_accuracy: 0.1600 - val_loss: 1.5276 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - categorical_accuracy: 0.4372 - loss: 1.3385\n",
      "Epoch 4: saving model to model_init_2024-12-3113_43_21.249773/model-00004-1.33790-0.44495-1.45060-0.29000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 820ms/step - categorical_accuracy: 0.4373 - loss: 1.3385 - val_categorical_accuracy: 0.2900 - val_loss: 1.4506 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622ms/step - categorical_accuracy: 0.4742 - loss: 1.2806\n",
      "Epoch 5: saving model to model_init_2024-12-3113_43_21.249773/model-00005-1.30999-0.46456-1.50859-0.22000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 742ms/step - categorical_accuracy: 0.4741 - loss: 1.2809 - val_categorical_accuracy: 0.2200 - val_loss: 1.5086 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658ms/step - categorical_accuracy: 0.4365 - loss: 1.2584\n",
      "Epoch 6: saving model to model_init_2024-12-3113_43_21.249773/model-00006-1.22360-0.47059-1.26608-0.43000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 783ms/step - categorical_accuracy: 0.4369 - loss: 1.2580 - val_categorical_accuracy: 0.4300 - val_loss: 1.2661 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663ms/step - categorical_accuracy: 0.5814 - loss: 1.0500\n",
      "Epoch 7: saving model to model_init_2024-12-3113_43_21.249773/model-00007-1.06848-0.57768-1.12124-0.55000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 788ms/step - categorical_accuracy: 0.5813 - loss: 1.0502 - val_categorical_accuracy: 0.5500 - val_loss: 1.1212 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - categorical_accuracy: 0.6178 - loss: 0.9800\n",
      "Epoch 8: saving model to model_init_2024-12-3113_43_21.249773/model-00008-0.91348-0.63801-1.12707-0.58000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 799ms/step - categorical_accuracy: 0.6181 - loss: 0.9793 - val_categorical_accuracy: 0.5800 - val_loss: 1.1271 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - categorical_accuracy: 0.7084 - loss: 0.7537\n",
      "Epoch 9: saving model to model_init_2024-12-3113_43_21.249773/model-00009-0.77021-0.71644-1.09280-0.59000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 786ms/step - categorical_accuracy: 0.7085 - loss: 0.7539 - val_categorical_accuracy: 0.5900 - val_loss: 1.0928 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625ms/step - categorical_accuracy: 0.7523 - loss: 0.7055\n",
      "Epoch 10: saving model to model_init_2024-12-3113_43_21.249773/model-00010-0.70533-0.73605-1.10528-0.53000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 749ms/step - categorical_accuracy: 0.7521 - loss: 0.7055 - val_categorical_accuracy: 0.5300 - val_loss: 1.1053 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640ms/step - categorical_accuracy: 0.6401 - loss: 0.9411\n",
      "Epoch 11: saving model to model_init_2024-12-3113_43_21.249773/model-00011-0.86114-0.67421-1.11444-0.60000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 759ms/step - categorical_accuracy: 0.6405 - loss: 0.9401 - val_categorical_accuracy: 0.6000 - val_loss: 1.1144 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - categorical_accuracy: 0.7571 - loss: 0.6831\n",
      "Epoch 12: saving model to model_init_2024-12-3113_43_21.249773/model-00012-0.63711-0.78884-0.95509-0.68000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 804ms/step - categorical_accuracy: 0.7575 - loss: 0.6826 - val_categorical_accuracy: 0.6800 - val_loss: 0.9551 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - categorical_accuracy: 0.8283 - loss: 0.5338\n",
      "Epoch 13: saving model to model_init_2024-12-3113_43_21.249773/model-00013-0.49152-0.84465-0.97418-0.66000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 796ms/step - categorical_accuracy: 0.8285 - loss: 0.5333 - val_categorical_accuracy: 0.6600 - val_loss: 0.9742 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630ms/step - categorical_accuracy: 0.8138 - loss: 0.5214\n",
      "Epoch 14: saving model to model_init_2024-12-3113_43_21.249773/model-00014-0.46481-0.83258-0.88675-0.73000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 750ms/step - categorical_accuracy: 0.8140 - loss: 0.5208 - val_categorical_accuracy: 0.7300 - val_loss: 0.8867 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611ms/step - categorical_accuracy: 0.8812 - loss: 0.3853\n",
      "Epoch 15: saving model to model_init_2024-12-3113_43_21.249773/model-00015-0.46025-0.84163-0.93353-0.74000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 735ms/step - categorical_accuracy: 0.8808 - loss: 0.3862 - val_categorical_accuracy: 0.7400 - val_loss: 0.9335 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612ms/step - categorical_accuracy: 0.8758 - loss: 0.3517\n",
      "Epoch 16: saving model to model_init_2024-12-3113_43_21.249773/model-00016-0.32691-0.88235-0.82695-0.75000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 728ms/step - categorical_accuracy: 0.8758 - loss: 0.3514 - val_categorical_accuracy: 0.7500 - val_loss: 0.8269 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615ms/step - categorical_accuracy: 0.9161 - loss: 0.4922\n",
      "Epoch 17: saving model to model_init_2024-12-3113_43_21.249773/model-00017-0.36048-0.90649-0.81404-0.73000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 740ms/step - categorical_accuracy: 0.9160 - loss: 0.4907 - val_categorical_accuracy: 0.7300 - val_loss: 0.8140 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624ms/step - categorical_accuracy: 0.9072 - loss: 0.3106\n",
      "Epoch 18: saving model to model_init_2024-12-3113_43_21.249773/model-00018-0.31100-0.90649-0.95109-0.73000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 745ms/step - categorical_accuracy: 0.9072 - loss: 0.3106 - val_categorical_accuracy: 0.7300 - val_loss: 0.9511 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671ms/step - categorical_accuracy: 0.9664 - loss: 0.1388\n",
      "Epoch 19: saving model to model_init_2024-12-3113_43_21.249773/model-00019-0.14858-0.95928-1.06964-0.72000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 802ms/step - categorical_accuracy: 0.9663 - loss: 0.1389 - val_categorical_accuracy: 0.7200 - val_loss: 1.0696 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625ms/step - categorical_accuracy: 0.9743 - loss: 0.1163\n",
      "Epoch 20: saving model to model_init_2024-12-3113_43_21.249773/model-00020-0.11227-0.97436-0.78710-0.80000.keras\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 752ms/step - categorical_accuracy: 0.9743 - loss: 0.1162 - val_categorical_accuracy: 0.8000 - val_loss: 0.7871 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=None,\n",
    "    initial_epoch=0\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d025b-e147-4a9d-bc7b-6e6973ed49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
